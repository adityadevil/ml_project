{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMs9iZUZXWZ00UZ8eBWF5FS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["##**Machine Learning Major Project:**\n","\n","###**Topic:**\n","\n","Image Classification using Convolutional Neural Networks (CNNs)\n","\n","###**Description:**\n","\n","The project involves building an image classification system using Convolutional Neural Networks (CNNs). CNNs are widely used for image-related tasks and have achieved remarkable success in various domains. This project will allow students to gain hands-on experience with deep learning, image processing, and model development.\n","\n","## Steps:\n","\n","### **1. Dataset Collection:**\n","Gather a large dataset of labeled images relevant to the chosen domain or application. You can explore publicly available datasets like CIFAR-10, ImageNet, or create your own dataset.\n","\n","### **2. Data Preprocessing:**\n","Preprocess the collected images by resizing them, normalizing pixel values, and splitting them into training and testing sets.\n","\n","###**3. CNN Model Architecture:**\n","Design and implement a CNN architecture suitable for image classification. Consider popular architectures like VGGNet, ResNet, or design your own architecture based on the problem requirements.\n","\n","###**4. Model Training:**\n","Train the CNN model using the training dataset. Experiment with different hyperparameters, such as learning rate, batch size, and optimizer, to achieve better performance.\n","\n","###**5. Model Evaluation:**\n","Evaluate the trained model using the testing dataset. Calculate metrics such as accuracy, precision, recall, and F1-score to assess the model's performance.\n","\n","###**6. Model Optimization:**\n","Fine-tune the model by applying techniques like data augmentation, regularization, or transfer learning to further improve performance.\n","\n","###**7. Deployment and Interface:**\n","Build a user-friendly interface where users can input images, and the trained model will predict the image class or category. You can use web frameworks like Flask or Django for this purpose.\n","\n","###**8. Documentation and Presentation:**\n","Document the project, including the dataset, preprocessing steps, model architecture, training process, evaluation results, and deployment instructions. Prepare a presentation to showcase the project's objectives, methodology, and outcomes."],"metadata":{"id":"nlIuiKKRYPSz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qK4-SYEYcWJZ","executionInfo":{"status":"ok","timestamp":1692912447951,"user_tz":-330,"elapsed":26470,"user":{"displayName":"Tensorflow","userId":"02052554466945344371"}},"outputId":"f5653175-d50b-4749-d586-403d4863e73f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoN9GORZ8rMv","executionInfo":{"status":"ok","timestamp":1692913338526,"user_tz":-330,"elapsed":874980,"user":{"displayName":"Tensorflow","userId":"02052554466945344371"}},"outputId":"bdcb8e7c-082c-48f8-f7d2-bd09f590e92a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 14s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 4s 0us/step\n","Epoch 1/10\n","1563/1563 [==============================] - 27s 11ms/step - loss: 1.4368 - accuracy: 0.4954 - val_loss: 1.2665 - val_accuracy: 0.5530\n","Epoch 2/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.2699 - accuracy: 0.5537 - val_loss: 1.2071 - val_accuracy: 0.5735\n","Epoch 3/10\n","1563/1563 [==============================] - 17s 11ms/step - loss: 1.2126 - accuracy: 0.5748 - val_loss: 1.1769 - val_accuracy: 0.5846\n","Epoch 4/10\n","1563/1563 [==============================] - 17s 11ms/step - loss: 1.1781 - accuracy: 0.5868 - val_loss: 1.1486 - val_accuracy: 0.5911\n","Epoch 5/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.1523 - accuracy: 0.5943 - val_loss: 1.1359 - val_accuracy: 0.5993\n","Epoch 6/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.1295 - accuracy: 0.6032 - val_loss: 1.1247 - val_accuracy: 0.6056\n","Epoch 7/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.1098 - accuracy: 0.6089 - val_loss: 1.1111 - val_accuracy: 0.6094\n","Epoch 8/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.0912 - accuracy: 0.6160 - val_loss: 1.1148 - val_accuracy: 0.6090\n","Epoch 9/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.0715 - accuracy: 0.6217 - val_loss: 1.1023 - val_accuracy: 0.6128\n","Epoch 10/10\n","1563/1563 [==============================] - 16s 10ms/step - loss: 1.0581 - accuracy: 0.6261 - val_loss: 1.0989 - val_accuracy: 0.6171\n","313/313 - 2s - loss: 1.0989 - accuracy: 0.6171 - 2s/epoch - 7ms/step\n","0.6171000003814697\n","313/313 [==============================] - 2s 7ms/step\n","Accuracy: 0.62\n","Precision: 0.62\n","Recall: 0.62\n","F1-score: 0.61\n","Epoch 1/10\n","1563/1563 [==============================] - 48s 28ms/step - loss: 1.3298 - accuracy: 0.1004 - val_loss: 0.9952 - val_accuracy: 0.1247\n","Epoch 2/10\n","1563/1563 [==============================] - 43s 27ms/step - loss: 1.0814 - accuracy: 0.0997 - val_loss: 0.8700 - val_accuracy: 0.1041\n","Epoch 3/10\n","1563/1563 [==============================] - 43s 27ms/step - loss: 0.9906 - accuracy: 0.1004 - val_loss: 0.8473 - val_accuracy: 0.0914\n","Epoch 4/10\n","1563/1563 [==============================] - 43s 28ms/step - loss: 0.9515 - accuracy: 0.1007 - val_loss: 0.8607 - val_accuracy: 0.0998\n","Epoch 5/10\n","1563/1563 [==============================] - 42s 27ms/step - loss: 0.9062 - accuracy: 0.0999 - val_loss: 0.8378 - val_accuracy: 0.1325\n","Epoch 6/10\n","1563/1563 [==============================] - 42s 27ms/step - loss: 0.8787 - accuracy: 0.1006 - val_loss: 0.7784 - val_accuracy: 0.0969\n","Epoch 7/10\n","1563/1563 [==============================] - 43s 27ms/step - loss: 0.8503 - accuracy: 0.0999 - val_loss: 0.7609 - val_accuracy: 0.0956\n","Epoch 8/10\n","1563/1563 [==============================] - 45s 29ms/step - loss: 0.8301 - accuracy: 0.1008 - val_loss: 0.7320 - val_accuracy: 0.0949\n","Epoch 9/10\n","1563/1563 [==============================] - 42s 27ms/step - loss: 0.8056 - accuracy: 0.1003 - val_loss: 0.7285 - val_accuracy: 0.0854\n","Epoch 10/10\n","1563/1563 [==============================] - 44s 28ms/step - loss: 0.7840 - accuracy: 0.1000 - val_loss: 0.7544 - val_accuracy: 0.0985\n","313/313 - 2s - loss: 0.7544 - accuracy: 0.0985 - 2s/epoch - 7ms/step\n","0.09849999845027924\n"]}],"source":["# Import necessary libraries\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","\n","# Load the CIFAR-10 dataset\n","(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n","\n","# Create the base model from the pre-trained model VGG16\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n","\n","# Freeze the base model\n","base_model.trainable = False\n","\n","# Add a new top layer\n","x = layers.Flatten()(base_model.output)\n","x = layers.Dense(512, activation='relu')(x)\n","x = layers.Dropout(0.5)(x)\n","predictions = layers.Dense(10, activation='softmax')(x)\n","\n","# This is the model we will train\n","model = models.Model(inputs=base_model.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(train_images, train_labels, epochs=10,\n","                    validation_data=(test_images, test_labels))\n","\n","# Evaluate the trained model on the test set\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","print(test_acc)\n","\n","\n","\n","# Evaluate the trained model on the test set\n","predictions = model.predict(test_images)\n","y_pred = np.argmax(predictions, axis=1)\n","accuracy = accuracy_score(test_labels, y_pred)\n","precision = precision_score(test_labels, y_pred, average='macro')\n","recall = recall_score(test_labels, y_pred, average='macro')\n","f1 = f1_score(test_labels, y_pred, average='macro')\n","\n","print(f'Accuracy: {accuracy:.2f}')\n","print(f'Precision: {precision:.2f}')\n","print(f'Recall: {recall:.2f}')\n","print(f'F1-score: {f1:.2f}')\n","\n","# Fine-tune the model using data augmentation and transfer learning\n","# Create a new base model from the pre-trained VGG16 model\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n","\n","# Freeze all layers in the base model except the last block of layers\n","for layer in base_model.layers[:-4]:\n","    layer.trainable = False\n","\n","# Add a new top layer to the model\n","x = layers.Flatten()(base_model.output)\n","x = layers.Dense(512, activation='relu')(x)\n","x = layers.Dropout(0.5)(x)\n","predictions = layers.Dense(10, activation='softmax')(x)\n","\n","# This is the new model we will fine-tune\n","model = models.Model(inputs=base_model.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Create an ImageDataGenerator for data augmentation\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True)\n","\n","# Fit the data generator on the training data\n","datagen.fit(train_images)\n","\n","# Fine-tune the model using the augmented data\n","history = model.fit(datagen.flow(train_images, train_labels),\n","                    epochs=10,\n","                    validation_data=(test_images, test_labels))\n","\n","# Evaluate the fine-tuned model on the test set\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","print(test_acc)\n","\n","# Save the trained model to an .h5 file\n","model.save('image_classification_model.h5')"]},{"cell_type":"markdown","source":["**Project Documentation: Image Classification with Transfer Learning\n","This documentation provides a comprehensive overview of the image classification project, encompassing dataset details, preprocessing steps, model architecture, training process, evaluation outcomes, and deployment instructions.**\n","\n","1.\tDataset: CIFAR-10\n","- The CIFAR-10 dataset consists of 60,000 color images, each sized 32x32 pixels, distributed across 10 distinct classes.\n","- The dataset is divided into a training set containing 50,000 images and a test set containing 10,000 images.\n","\n","2.\tPreprocessing:\n","- Preprocessing involves pixel normalization, which scales pixel values within the range of [0, 1].\n","  \n","3.\tModel Architecture:\n","- The foundational model is VGG16, pretrained on the ImageNet dataset. It excludes the fully connected layers.\n","- Additional layers include: Flatten, Dense (512 units, ReLU activation), Dropout (rate = 0.5), Dense (10 units, softmax activation).\n","\n","4.\tTraining Process:\n","- Initialization involves loading pretrained VGG16 weights and keeping them frozen.\n","- The complete model is compiled using the Adam optimizer and sparse categorical cross-entropy loss.\n","- Initial training spans 10 epochs on the training set, with validation data for monitoring progress.\n","- Subsequent fine-tuning involves data augmentation.\n","\n","5.\tEvaluation Results:\n","- After each training phase, the model's performance is evaluated on the test set.\n","- The test accuracy achieved after the initial training phase is approximately 61.71%.\n","- After fine-tuning, the accuracy decreases to around 9.84% due to significant modifications in the model architecture and the freezing of certain layers.\n","  - For the initial model:\n","  - Precision: 0.62\n","  - Recall: 0.62\n","  - F1-score: 0.61\n","\n","6.\tDeployment Instructions:\n","- To deploy the model for inference, it's essential to save the model architecture and weights.\n","- Loading the saved model and employing it for making predictions are the fundamental deployment steps.\n","- Make sure to have the requisite libraries (TensorFlow, scikit-learn) installed for a seamless deployment experience.\n","- The deployment avenues include cloud platforms, mobile applications, and web services based on specific project requirements.\n","\n","Output Summary:\n"," - Initial model accuracy on the test set: ~61.71%\n"," - Accuracy after fine-tuning: ~9.84%\n"," - For the initial model:\n"," - Precision: 0.62\n"," - Recall: 0.62\n"," - F1-score: 0.61\n"],"metadata":{"id":"Un3dPLUCaDNI"}}]}